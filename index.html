<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Rishab K Pattnaik | AI Portfolio with Smart Assistant</title>
    
    <!-- Enhanced SEO -->
    <meta name="description" content="AI Research Engineer specializing in Deep Learning, Medical Imaging, and Computer Vision with intelligent AI assistant">
    <meta name="keywords" content="AI research, deep learning, medical imaging, computer vision, machine learning portfolio, chatbot, AI assistant">
    <meta name="author" content="Rishab K Pattnaik">
    
    <!-- Open Graph -->
    <meta property="og:title" content="Rishab K Pattnaik | AI Research Portfolio with Smart Assistant">
    <meta property="og:description" content="Innovative AI solutions with Netflix-quality research and development, featuring an intelligent chatbot assistant">
    <meta property="og:type" content="website">
    <meta property="og:image" content="https://storage.googleapis.com/kaggle-avatars/images/21226742-kg.jpg">
    
    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Rishab K Pattnaik | AI Portfolio">
    <meta name="twitter:description" content="AI Research Engineer with intelligent portfolio assistant">
    
    <!-- Fonts & Icons -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Netflix+Sans:wght@300;400;500;600;700;800&family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Enhanced CSS -->
    <link rel="stylesheet" href="styles.css">
    
    <!-- PWA Manifest -->
    <link rel="manifest" href="manifest.json">
    
    <!-- Preload critical resources -->
    <link rel="preload" href="styles.css" as="style">
    <link rel="preload" href="script.js" as="script">
    
    <!-- Favicon and PWA Icons -->
    <link rel="icon" type="image/x-icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>🤖</text></svg>">
    <link rel="apple-touch-icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><circle cx='50' cy='50' r='40' fill='%23e50914'/><text x='50' y='65' font-size='40' text-anchor='middle' fill='white'>🤖</text></svg>">
    
    <!-- PWA specific meta tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="apple-mobile-web-app-title" content="AI Portfolio">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="msapplication-TileColor" content="#141414">
    <meta name="theme-color" content="#e50914">
</head>

<body>
    <!-- Loading Overlay -->
    <div class="loading-overlay" id="loadingOverlay">
        <div class="loading-spinner"></div>
    </div>

    <!-- Enhanced Netflix Header -->
    <header class="netflix-header" id="header">
        <nav class="nav-container">
            <a href="#home" class="logo">Rishab</a>
            
            <ul class="nav-menu">
                <li><a href="#home" class="active">Home</a></li>
                <li><a href="#about">About</a></li>
                <li><a href="#experience">Experience</a></li>
                <li><a href="#research">Research</a></li>
                <li><a href="#project">Projects</a></li>
                <li><a href="#skills">Skills</a></li>
                <li><a href="#education">Education</a></li>
                <li><a href="#blogs">Blogs</a></li>
            </ul>
            
            <div class="nav-actions">
                <button class="theme-toggle" id="themeToggle">
                    <i class="fas fa-sun"></i>
                    <span>Toggle Theme</span>
                </button>
            </div>
        </nav>
    </header>

    <!-- Enhanced Hero Section with Fixed Profile Photo and Animated Subtitle -->
    <section id="home" class="hero-section">
        <div class="hero-particles">
            <div class="particle"></div>
            <div class="particle"></div>
            <div class="particle"></div>
        </div>

        <div class="hero-content">
            <!-- Left side: Photo and Badge -->
            <div class="hero-left">
                <figure class="profile-photo">
                    <img src="https://storage.googleapis.com/kaggle-avatars/images/21226742-kg.jpg" 
                         alt="Rishab K Pattnaik - AI Research Engineer"
                         width="250" 
                         height="250">
                </figure>

                <div class="hero-badge">
                    <i class="fas fa-brain"></i>
                    <span>AI Research Engineer</span>
                </div>
            </div>

            <!-- Right side: Title and Subtitle -->
            <div class="hero-right">
                <h1 class="hero-title">Rishab K Pattnaik</h1>
                <!-- Animated subtitle with typing effect -->
                <p class="hero-subtitle">
                    <span class="typing-text" id="typingText"></span>
                </p>

                <div class="social-buttons">
                    <a href="https://drive.google.com/file/d/1VWgnytIs7XbztODSluR1ywleWitUEg8W/view?usp=sharing" target="_blank" class="social-btn" aria-label="Download Resume">
                        <i class="fas fa-file-alt"></i>
                        <span>Resume</span>
                    </a>
                    <a href="https://github.com/Rishab27279" target="_blank" class="social-btn" aria-label="GitHub Profile">
                        <i class="fab fa-github"></i>
                        <span>GitHub</span>
                    </a>
                    <a href="https://www.linkedin.com/in/rishab-k-pattnaik-6a9939249/" target="_blank" class="social-btn" aria-label="LinkedIn Profile">
                        <i class="fab fa-linkedin"></i>
                        <span>LinkedIn</span>
                    </a>
                    <a href="https://medium.com/@rishab27279" target="_blank" class="social-btn" aria-label="Medium Blog">
                        <i class="fab fa-medium"></i>
                        <span>Blog</span>
                    </a>
                    <a href="/cdn-cgi/l/email-protection#56306466646466626f67163e2f3233243734373278343f22257b263f3a37383f783735783f38" class="social-btn" aria-label="Send Email">
                        <i class="fas fa-envelope"></i>
                        <span>Email</span>
                    </a>
                </div>
            </div>
        </div>
    </section>

    <!-- About Section -->
    <section id="about" class="section scroll-animation">
        <h2 class="section-title">Who I Am</h2>
        
        <p class="about-description" style="font-size: 1.3em; line-height: 1.6;">
            <span class="hover-blue">A</span> <span class="hover-blue">Machine</span> <span class="hover-blue">Learning</span> <span class="hover-blue">Engineer</span> <span class="hover-blue">and</span> <span class="hover-blue">Electronics</span> <span class="hover-blue">&</span> <span class="hover-blue">Communication</span> <span class="hover-blue">Engineering</span> <span class="hover-blue">student</span> <span class="hover-blue">at</span> <span class="hover-blue">BITS</span> <span class="hover-blue">Pilani</span> <span class="hover-blue">Hyderabad</span> <span class="hover-blue">Campus,</span> <span class="hover-blue">passionate</span> <span class="hover-blue">about</span> <span class="hover-blue">developing</span> <span class="hover-blue">AI</span> <span class="hover-blue">solutions</span> <span class="hover-blue">for</span> <span class="hover-blue">real-world</span> <span class="hover-blue">challenges.</span> <span class="hover-blue">I</span> <span class="hover-blue">focus</span> <span class="hover-blue">on</span> <span class="hover-blue">medical</span> <span class="hover-blue">imaging</span> <span class="hover-blue">through</span> <span class="hover-blue">deep</span> <span class="hover-blue">learning</span> <span class="hover-blue">architectures</span> <span class="hover-blue">and</span> <span class="hover-blue">wavelet-CNN</span> <span class="hover-blue">integration,</span> <span class="hover-blue">contributing</span> <span class="hover-blue">to</span> <span class="hover-blue">published</span> <span class="hover-blue">research</span> <span class="hover-blue">with</span> <span class="hover-blue">measurable</span> <span class="hover-blue">diagnostic</span> <span class="hover-blue">improvements.</span> <span class="hover-blue">My</span> <span class="hover-blue">work</span> <span class="hover-blue">spans</span> <span class="hover-blue">medical</span> <span class="hover-blue">AI</span> <span class="hover-blue">applications</span> <span class="hover-blue">to</span> <span class="hover-blue">intelligent</span> <span class="hover-blue">document</span> <span class="hover-blue">processing</span> <span class="hover-blue">systems.</span> <span class="hover-blue">I</span> <span class="hover-blue">explore</span> <span class="hover-blue">cutting-edge</span> <span class="hover-blue">architectures</span> <span class="hover-blue">like</span> <span class="hover-blue">Vision</span> <span class="hover-blue">Transformers</span> <span class="hover-blue">and</span> <span class="hover-blue">Mamba,</span> <span class="hover-blue">building</span> <span class="hover-blue">practical</span> <span class="hover-blue">solutions</span> <span class="hover-blue">using</span> <span class="hover-blue">PyTorch</span> <span class="hover-blue">and</span> <span class="hover-blue">TensorFlow.</span> <span class="hover-blue">Through</span> <span class="hover-blue">technical</span> <span class="hover-blue">blogs,</span> <span class="hover-blue">I</span> <span class="hover-blue">share</span> <span class="hover-blue">knowledge</span> <span class="hover-blue">bridging</span> <span class="hover-blue">theory</span> <span class="hover-blue">with</span> <span class="hover-blue">practice.</span>
        </p>
    </section>

    <!-- Experience Section -->
    <section id="experience" class="section scroll-animation">
        <h2 class="section-title">Experience</h2>
        <div class="experience-container">
            <!-- Hamad Medical Corporation -->
            <div class="experience-box scroll-animation"
                 data-popup-trigger="experience-popup-3"
                 data-heading="Junior Research Assistant (Intern)"
                 data-subheading="Dept. of Surgery, Hamad Medical Corporation, Doha, Qatar"
                 data-body="<style>
:root {
  --ai-blue: #00cfff;
}
h3 {
  margin-bottom: 20px;
  margin-top: 20px;
  color: var(--ai-blue);
}
</style>
</head>
<body>

<p>Current work focuses on critical areas within AI in healthcare, particularly the development of novel frameworks for emergency triage systems and the improved diagnosis of Chronic Kidney Disease (CKD).</p>

<h3>Novel Framework for Emergency Triage Systems</h3>
<ul>
    <li>Triage is a cornerstone of emergency medicine requiring precise human attention. The inherent need for rapid and accurate patient prioritization makes it a prime candidate for automation, now feasible through advancements in artificial intelligence.</li>
    <li>Efforts are dedicated to pioneering innovative approaches and models, particularly leveraging multi-modal data, to effectively address the complexities inherent in emergency triage.</li>
    <li>This development process encompasses creating and training innovative models utilizing advanced pipelines. These pipelines integrate techniques such as learning rate scheduler restarts, warm-up periods, various regularization methods, and comprehensive hyperparameter tuning to optimize performance.</li>
</ul>

<h3>Enhanced Diagnosis of Chronic Kidney Disease (CKD)</h3>
<ul>
    <li>A second area of focus involves addressing Chronic Kidney Disease (CKD) by integrating multiple clinical datasets.</li>
    <li>The primary objective is to forge more robust models for enhanced diagnosis, aiming to identify risks before a rapid decline in the estimated Glomerular Filtration Rate (eGFR), a key indicator of kidney function.</li>
</ul>"
                 data-github="  ">
                
                <div class="project-image">
                    <img src="https://media.licdn.com/dms/image/v2/C4E1BAQHDEOfmMYyesQ/company-background_10000/company-background_10000/0/1584506822052?e=2147483647&v=beta&t=ZDxV2JaqSq9g8mYw_b0EFzy_-zJZnbd50KstBUsAoVo" alt="Hamad Medical Corporation">
                </div>
                
                <div class="project-content">
                    <div class="project-subtitle">Hamad Medical Corporation, Qatar</div>
                    <h3><i class="fa fa-medkit"></i>AI Research Intern</h3>
                    <p>Currently engaged in Emergency Research within the Department of Surgery at Hamad Medical Corporation, under the guidance of Dr. Sarada Prasad Dakua, Principal Data Scientist. The work encompasses developing and refining AI-driven approaches to enhance patient triage and clinical decision-making in high-pressure emergency settings. This involvement aims to improve both clinical outcomes and operational efficiency by leveraging advanced data science techniques tailored to emergency care challenges. </p>
                    <div class="experience-date-box">05/2025 - Present</div>
                </div>
            </div>
          
            <!-- Research Assistant -->
            <div class="experience-box scroll-animation" 
                 data-popup-trigger="experience-popup-1"
                 data-heading="Research Assistantship in Advanced Medical Diagnosis using Deep Learning 🩺"
                 data-subheading="Department of Electrical and Communication Engineering, BITS Pilani Hyderabad Campus"
                 data-body="<style>
:root {
  --ai-blue: #00cfff;
}
h3 {
  margin-bottom: 20px;
  margin-top: 20px;
  color: var(--ai-blue);
}
</style>
</head>
<body>

<h2>Publications</h2>

<h3>Journals</h3>
<ul>
    <li>Rishab K Pattnaik and RK Tripathy, '<b>Multi-Frequency Aware Deep Representation Learning for Automated Detection of Bone Fractures using muscle X-ray Images</b>'. (Submitted for Review)</li>
</ul>

<h3>Book Chapters</h3>
<ul>
    <li>Rishab K Pattnaik, RK Tripathy, '<b>Deep Representation Learning for Computer-Aided Detection of Pneumonia and Tuberculosis Using Chest X-Ray Images</b>', Non-stationary and nonlinear data processing for automated computer-aided medical diagnosis, Elsevier book, 2025 (ISBN: 9780443314261)</li>
</ul>

<h3>Technical Contributions & Research Focus</h3>

<ul>
    <li>
        Advanced Medical Imaging & Deep Learning Architectures: Developed and applied sophisticated deep learning architectures, integrating wavelet-DNN techniques for enhanced medical image analysis. These methods have consistently achieved **5-15% accuracy improvements** in abnormality detection across diverse datasets, encompassing over 10,000 medical scans.
    </li>
    <li>
        Transfer Learning and Custom Architecture Development: Leveraged state-of-the-art architectures, including EfficientNetV2B2 and ResNet50, as foundational models to enable rapid knowledge transfer from general image recognition to specialized medical applications. Simultaneously, designed bespoke Convolutional Neural Network (CNN) architectures with strategically configured multi-layer networks, specifically addressing the unique characteristics of medical imaging data through specialized frameworks.
    </li>
    <li>
        Deep Representation Learning & Wavelet Transform Integration: Enhanced diagnostic accuracy by implementing advanced deep representation learning techniques to create robust feature representations for complex medical abnormality detection. Furthermore, incorporated wavelet transform methodologies into neural network frameworks, significantly improving model sensitivity to subtle pathological indicators.
    </li>
    <li>
        Transformer Architecture Application: Utilized transformer-based architectures equipped with attention mechanisms to maintain comprehensive contextual awareness across entire medical images, crucial for accurate diagnosis.
    </li>
    <li>
        Current Research Focus: Actively exploring diffusion models for generating synthetic medical imaging data to augment existing datasets and investigating advanced deep representation learning techniques. A key area of investigation also includes improving cross-modal generalizability across varying imaging modalities. Also focus on newer state of the art models like Mamba/Vision Mamba, xLSTM, etc.
    </li>
</ul>"
                 data-github="https://github.com/Riiishaab/OS-Detection-">
                
                <div class="project-image">
                    <img src="https://www.vidyavision.com/CollegeUploads/Photos/2019-21-8-12-08-53_brila.jpg" alt="BITS Pilani Medical AI Research">
                </div>
                
                <div class="project-content">
                    <div class="project-subtitle">BITS Pilani Hyderabad</div>
                    <h3><i class="fa fa-graduation-cap"></i>Research Assistant</h3>
                    <p>As a Research Assistant in the Department of ECE at BITS Pilani, I contributed to pioneering research in medical imaging under the supervision of Dr. Rajesh Kumar Tripathy.
It involves developing and applying novel deep learning models, including advanced CNNs and transformer architectures , alongside the innovative integration of wavelet-DNN techniques to enhance diagnostic accuracy from medical scans .</p>
                    <div class="experience-date-box">08/2024 - Present</div>
                </div>
            </div>

            <!-- Research Intern -->
            <div class="experience-box scroll-animation"
                 data-popup-trigger="experience-popup-2"
                 data-heading="Computer Vision Research Intern 🔬"
                 data-subheading="Indira Gandhi Center of Atomic Research, Dept. of Atomic Energy, Govt of India"
                 data-body="<style>

h2 {
  margin-bottom: 15px;
}
h3 {
  margin-bottom: 20px;
  margin-top: 20px;
  color: var(--ai-blue);
}
</style>
</head>
<body>

<h2>Internship Project: Camouflaged Object Detection</h2>
<p>
During this internship, the primary focus was on advancing solutions for the challenging problem of camouflaged object detection (COD). This was achieved by leveraging and fine-tuning Meta AI's Segment Anything Model (SAM), a powerful vision transformer. The project aimed to develop robust methods for detecting objects that blend seamlessly with their surroundings, a task where traditional computer vision approaches often fall short.
</p>

<h3>Project Implementation & Fine-Tuning</h3>
<ul>
    <li>
        <strong>Dataset and Environment:</strong> Utilized the COD10K dataset, a standard benchmark for camouflaged objects. A custom data pipeline was implemented, including a specialized dataset class and dataloader, to facilitate seamless integration with the SAM architecture.
    </li>
    <li>
        <strong>Baseline Analysis:</strong> A baseline was established by evaluating the zero-shot performance of the pre-trained SAM model. This analysis confirmed its limitations in the COD domain, aligning with recent research findings that foundation models require adaptation for specialized tasks.
    </li>
    <li>
        <strong>Fine-Tuning Methodology:</strong> A sophisticated fine-tuning strategy was developed using a bounding box prompt-based approach to guide SAM’s attention. This involved creating a custom training engine to manage epochs, optimize hyperparameters, and implement targeted loss functions, drawing on experience with model architecture and regularization techniques.
    </li>
</ul>

<h3>Technical Innovations & Quantified Impact</h3>
<ul>
    <li>
        <strong>Enhanced Detection Accuracy:</strong> By adapting SAM's promptable segmentation capabilities with a box-prompt guided strategy, the model's detection accuracy was significantly improved. This resulted in a <strong>14% increase</strong> in mean Intersection over Union (mIoU) scores compared to the baseline.
    </li>
    <li>
        <strong>Improved Model Sensitivity:</strong> The fine-tuned model demonstrated superior performance in identifying hard-to-detect objects, achieving a <strong>22% reduction</strong> in false negative rates and an <strong>18% enhancement</strong> in edge precision for more accurate boundary delineation.
    </li>
    <li>
        <strong>Efficient Performance:</strong> Despite the extensive fine-tuning and architectural adaptations, the model's inference speed was maintained within 1.2x of the original SAM, ensuring practical applicability. The improvements were consistent across various camouflage types within the COD10K dataset.
    </li>
</ul>"
                 data-github="https://github.com/Riiishaab/Camouflaged_Object_Detection/tree/main">
                
                <div class="project-image">
                    <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRID3vHg_z_e9b89bCHbUV9dLIfyQA7TTFt93RGlnVuO9f5vfO4FHNzv7peicgkR0zthmM&usqp=CAU" alt="IGCAR Computer Vision Research">
                </div>
                
                <div class="project-content">
                    <div class="project-subtitle">IGCAR Kalpakkam (On-Site)</div>
                    <h3><i class="fa fa-laptop"></i>CV Research Intern</h3>
                    <p>As a Research Intern at Indira Gandhi Center of Atomic Research, Kalpakkam, under the supervision of Raja Sekhar M (SO/E), work focused on advancing camouflaged object detection by fine-tuning Meta's Segment Anything Model (SAM) for improved identification of objects in complex visual environments. This role involved designing and implementing specialized training strategies to adapt state-of-the-art vision transformer models for challenging detection tasks.</p>
                    <div class="experience-date-box">05/2024 - 08/2024</div>
                </div>
            </div>
        </div>
    </section>

    <!-- Research Section -->
    <section id="research" class="section scroll-animation">
        <h2 class="section-title">Research & Publications</h2>
        <div class="research-container">
                      <div class="research-box scroll-animation"
                 data-popup-trigger="research-popup-2"
                 data-heading="Journal Paper: Multi-Frequency Aware Deep Representation Learning
Framework for Automated Detection of Bone Fractures
using Muscle X-ray Images"
                 data-subheading="Automated Bone Fracture Detection"
                 data-body="<style>

h3 {
  margin-bottom: 20px;
  margin-top: 20px;
  color: var(--ai-blue);
}
</style>
</head>
<body>
                            
<ul>
    <li>Authored a research paper under Dr. Rajesh Kumar Tripathy detailing a novel approach for the automated detection of bone fractures from Muscle X-ray (MXR) images, a critical task for improving patient outcomes.</li>
    <li>Proposed the <strong>Multi-Frequency Aware Deep Representation Learning Network (MFADRLN)</strong>, an innovative framework that integrates multi-resolution analysis using a Novel Signal Processing technique (Can't specify as its the novelty and its under Review!) with deep representation learning.</li>
    <li>The model's architecture leverages a powerful <strong>EfficientNetV2B2-based</strong> feature extractor, applying it to different frequency sub-bands of the X-ray image to capture more comprehensive diagnostic features.</li>
    <li>Achieved a benchmark accuracy of <strong>92.22%</strong> and an F1-score of <strong>0.841</strong> on publicly available musculoskeletal X-ray databases, demonstrating high efficacy and robustness.</li>
    <li>The proposed MFADRLN model demonstrated superior performance against multiple state-of-the-art transfer learning models (including ResNet50, DenseNet201) and vision transformer architectures (ViT, SwiT).</li>
    <li>The model has been successfully deployed within a web application, demonstrating its potential for IoT-enabled, real-world automated fracture detection.</li>
</ul>
<p><em>As this research is currently undergoing peer review, further specific details of the methodology and results cannot be provided at this time.</em></p>"
                 data-github="">
                
                <div class="project-image">
                    <img src="https://miro.medium.com/v2/resize:fit:1400/1*rTH077SafntwoU1puQKfug.jpeg"alt="Medical Research Journal">
                </div>
                
                <div class="project-content">
                    <div class="project-subtitle">Journal Paper</div>
                    <h3><i class="fas fa-microscope"></i>Multi-Frequency Aware Deep Representation Learning
Framework for Automated Detection of Bone Fractures
using Muscle X-ray Images</h3>
                    <p>A Journal Paper authored under the supervision of Dr. Rajesh Kumar Tripathy focuses on developing advanced deep learning methodologies for medical imaging applications, specifically targeting the automated detection and diagnosis of complex medical conditions using multi-modal data. This research aims to enhance diagnostic accuracy and clinical decision support through innovative model architectures and training techniques. The paper is currently under review and has not yet been published yet.</p>
                </div>
            </div>
                                                    
            <div class="research-box scroll-animation"
                 data-popup-trigger="research-popup-1"
                 data-heading="Book Chapter: Deep Learning Applications in Medical Imaging"
                 data-subheading="Automated Detection and Classification of Respiratory Diseases Using Chest X-Ray Analysis"
                 data-body="<h2>Disease Overview</h2><h3>Pneumonia (PN)</h3><ul><li>Type of respiratory disease affecting millions annually</li><li>Caused by bacterial and viral infections</li><li>Requires accurate and early detection for effective treatment</li></ul><h3>Tuberculosis (TB)</h3><ul><li>Bacterial infection primarily affecting lungs and other body parts</li><li>Global health concern with significant impact</li><li>Early diagnosis crucial for preventing complications</li></ul><h2>Diagnostic Approach</h2><h3>Chest X-Ray (CXR) Imaging</h3><ul><li>Widely used diagnostic modality for TB and PN detection</li><li>Non-invasive and accessible imaging technique</li><li>Standard practice in respiratory disease diagnosis</li></ul><h3>Computer-Aided Diagnosis (CAD) Systems</h3><ul><li>Automated analysis of CXR images</li><li>Assists radiologists in accurate disease diagnosis</li><li>Reduces human error and improves diagnostic efficiency</li></ul><h2>Deep Representation Learning (DRL) Framework</h2><h3>Network Architecture</h3><ul><li>Transfer learning blocks utilized as feature extractors</li><li>Multiple dense layers for feature processing</li><li>Final classification layer for disease categorization</li></ul><h3>Performance Evaluation</h3><ul><li>Tested on CXR images from public databases</li><li><strong>87.08% overall accuracy</strong> achieved</li><li>Capable of classifying viral PN, bacterial PN, and TB diseases</li></ul><h2>Implementation and Deployment</h2><h3>Cloud-Based Framework</h3><ul><li>DRL network deployed on cloud infrastructure</li><li>Internet of Things (IoT)-enabled detection system</li><li>Scalable and accessible diagnostic solution</li></ul><h3>Future Directions</h3><ul><li>Development of CAD systems in federated learning environments</li><li>Integration with IoT-based healthcare networks</li><li>Enhanced privacy-preserving diagnostic capabilities</li></ul>"
                 data-github="https://github.com/Riiishaab/deep-representation-learning">
                
                <div class="project-image">
                    <img src="https://newchennaipublications.com/sites/newchennaipub/files/books.jpg" alt="Elsevier Research Publication">
                </div>
                
                <div class="project-content">
                    <div class="project-subtitle">Elsevier Book Chapter</div>
                    <h3><i class="fas fa-book"></i>Deep Representation Learning for Computer-Aided Detection of Pneumonia and Tuberculosis Using Chest X-Ray Images</h3>
                    <p>Published research in Elsevier's book "Non-stationary and nonlinear data processing for automated computer-aided medical diagnosis" authored by RK Tripathy, RB Pachori, Sibashankar Padhy, Maarten De Vos (ISBN:9780443314261).</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Projects Section -->
    <section id="project" class="section scroll-animation">
        <h2 class="section-title">Featured Projects</h2>
        <div class="project-container">
            <!-- EdgeSeg-AI -->
            <div class="project-box scroll-animation"
                 data-popup-trigger="project-popup-7"
                 data-heading="EdgeSeg-AI"
                 data-subheading="Making Advanced Image Segmentation Accessible to Everyone 🔍"
                 data-body="<style>
                            h3 {
  margin-bottom: 20px;
  margin-top: 20px;
  color: var(--ai-blue);
}
</style>
                            <p><strong>EdgeSeg-AI</strong> is a revolutionary framework that makes advanced image segmentation accessible to everyone by introducing a novel, resource-efficient approach to prompt-based image segmentation. The framework addresses the computational limitations of cutting-edge segmentation methods by sequentially orchestrating three specialized models: Large Language Model (LLM), Fine-tuned VLM and Segment Anything Model (SAM). This innovative architecture achieves a 60-70% reduction in peak memory usage while maintaining high segmentation quality.</p>

<p>----------------------------------------------------------------------------------------------------------------</p>

<p>GitHub Repository: https://github.com/Rishab27279/EdgeSeg-AI</p>

<p>----------------------------------------------------------------------------------------------------------------</p>

<h3>Core Innovation</h3>
<p>EdgeSeg-AI presents a unique architectural approach that sequentially orchestrates three specialized models, representing a fundamental shift from conventional approaches that load all models simultaneously:</p>
<ul>
<li><strong>Large Language Model (LLM)</strong>: Complex prompt interpretation and simplification</li>
<li><strong>Fine-tuned Florence-2 (VLM)</strong>: Precise object detection from natural language descriptions</li>
<li><strong>Segment Anything Model (SAM)</strong>: High-quality mask generation</li>
<li><strong>Memory Efficiency</strong>: 60-70% reduction in peak memory usage while maintaining segmentation quality</li>
</ul>

<h3>Technical Architecture</h3>
<p>The framework operates through a carefully designed three-stage pipeline that maximizes efficiency and accessibility:</p>
<ul>
<li><strong>Prompt Simplification</strong>: Complex natural language queries are processed and refined for optimal object detection</li>
<li><strong>Object Detection</strong>: Florence-2 generates precise bounding boxes from simplified prompts</li>
<li><strong>Segmentation</strong>: SAM produces detailed masks using the detected bounding boxes as guidance</li>
</ul>

<h3>Key Technical Advantages</h3>
<ul>
<li><strong>Interactive Prompt Refinement</strong>: User control and transparency in the segmentation process</li>
<li><strong>Memory-Efficient Sequential Loading</strong>: Revolutionary approach to model orchestration</li>
<li><strong>Maintained Accuracy</strong>: High segmentation quality despite resource constraints</li>
<li><strong>Consumer Hardware Compatible</strong>: Works with standard 4GB+ GPU memory</li>
</ul>

<h3>Performance Benchmarks</h3>
<p>EdgeSeg-AI demonstrates significant improvements in resource efficiency while maintaining segmentation quality:</p>
<ul>
<li><strong>Peak Memory Usage</strong>: Reduced from 12-16GB to 4-6GB (60-70% reduction)</li>
<li><strong>Minimum GPU Memory</strong>: Reduced from 8GB to 4GB (50% reduction)</li>
<li><strong>Processing Time</strong>: 3-5 minutes vs 1 minute (trade-off for accessibility)</li>
<li><strong>Segmentation Quality</strong>: High quality maintained across all metrics</li>
</ul>

<h3>Real-World Applications</h3>
<p>The framework demonstrates robust performance across diverse contexts, showcasing the potential for democratizing AI-powered image analysis:</p>
<ul>
<li><strong>Waste Management</strong>: Environmental applications ('Where should I throw the wrapper?')</li>
<li><strong>Automotive Interfaces</strong>: Vehicle dashboard analysis ('Where should I look to know the speed?')</li>
<li><strong>Multi-Domain Versatility</strong>: Adaptable across various industrial and consumer applications</li>
<li><strong>Accessibility Focus</strong>: Designed for widespread adoption and implementation</li>
</ul>

<h3>Research Community Engagement</h3>
<p>This work builds upon the foundational contributions of the LLM-Seg paper by Junchi Wang and Lei Ke from ETH Zurich, adapting their approach to prioritize computational efficiency and accessibility. The research community's insights are invaluable for advancing this work further in the following areas:</p>
<ul>
<li><strong>Benchmarking</strong>: Against additional state-of-the-art methods</li>
<li><strong>Dataset Evaluation</strong>: Testing on diverse datasets and use cases</li>
<li><strong>Memory Optimization</strong>: Strategies for further memory reduction</li>
<li><strong>Novel Applications</strong>: Implementation in resource-constrained environments</li>
</ul>

<h3>Open Source Commitment</h3>
<ul>
<li><strong>Full Open Source</strong>: Complete codebase available for exploration and contribution</li>
<li><strong>Community Driven</strong>: Open for critical evaluation and collaborative improvement</li>
<li><strong>Accessibility Focus</strong>: Democratizing advanced AI capabilities for everyone</li>
<li><strong>Research Transparency</strong>: Supporting reproducible research and innovation</li>
</ul>

<p>EdgeSeg-AI represents a significant advancement in making sophisticated image segmentation technology accessible to a broader audience, combining cutting-edge AI research with practical resource efficiency. The framework's innovative sequential model loading approach opens new possibilities for deploying advanced computer vision capabilities on standard consumer hardware, democratizing access to powerful AI tools across various domains and applications.</p>"
                 data-github="https://github.com/Rishab27279/EdgeSeg-AI">
                
                <div class="project-image">
                    <img src="https://ik.imagekit.io/lbiij6kvl/bbb.png?updatedAt=1752259736090" alt="Gender Detection System">
                </div>
                
                <div class="project-content">
                    <div class="project-subtitle">Research Paper Productionization</div>
                    <h3><i class="fa fa-user"></i>EdgeSeg-AI</h3>
                    <p>EdgeSeg-AI is a research backed image segmentation framework that democratizes advanced AI by reducing memory usage 60-70% through sequential model loading. It uses Multi-Modal Architecture, which intelligently orchestrates prompt interpretation, object detection, and mask generation for seamless user experience which enables high-quality segmentation on consumer hardware.</p>
                </div>
            </div>
          
             <!-- AI Document Processing -->
            <div class="project-box scroll-animation"
                 data-popup-trigger="project-popup-8"
                 data-heading="Moody.AI"
                 data-subheading="🎭 Multimodal Emotion Recognition Engine 🚀"
                 data-body="<style>
h3 {
  margin-bottom: 20px;
  margin-top: 20px;
  color: var(--ai-blue);
}
</style>

<p><strong>Moody.AI</strong> is a cutting-edge multimodal AI system that analyzes emotions from video content using computer vision, audio processing, and natural language processing. Powered by state-of-the-art deep learning models including <strong>DINOv2, Wav2Vec2, DistilBERT, and Whisper</strong>, the system provides comprehensive sentiment analysis with an intuitive web interface and achieves <strong>61% accuracy</strong> on the challenging MELD dataset.</p>
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

<p>Docker Hub: https://hub.docker.com/r/rishab27279/moody-ai | GitHub: https://github.com/Rishab27279/MoodyAI</p>
                            
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

<h3>System Features</h3>
<ul>
<li>Multimodal analysis with simultaneous processing of vision, audio, and text</li>
<li>Real-time emotion classification with interactive confidence scores</li>
<li>Production-ready containerized deployment with Docker</li>
<li>Modern glass morphism UI with animated visualizations</li>
<li>Supports video files (MP4, MOV, MKV) up to 200MB</li>
<li>Fast processing: 10-30 seconds per video on GPU</li>
</ul>

<h3>Model Architecture and Performance</h3>
<p>Moody.AI employs a sophisticated trimodal fusion architecture combining multiple AI models:</p>
<ul>
<li><strong>Vision Processing</strong>: DINOv2 ViT-B/14 for facial expression analysis with 768-dimensional embeddings compressed to 256 dimensions</li>
<li><strong>Audio Processing</strong>: Fine-Tuned-Wav2Vec2-base Model for emotional audio representations achieving 32% accuracy on MELD</li>
<li><strong>Text Processing</strong>: Fine-tuned DistilBERT-base-uncased achieving 50% accuracy on MELD dataset</li>
<li><strong>Speech-to-Text</strong>: OpenAI Whisper for multilingual transcription with automatic language detection</li>
</ul>

<h3>Trimodal Fusion Innovation</h3>
<p>The breakthrough architecture combines all three modalities through advanced fusion techniques:</p>
<ul>
<li><strong>Vision Feature Compression</strong>: Novel noise reduction technique that improved accuracy from 45-49% to 61%</li>
<li><strong>Cross-Modal Attention</strong>: Advanced fusion mechanisms enabling interaction between text, audio, and vision features</li>
<li><strong>Progressive Architecture</strong>: Systematic evolution from bimodal (56-57% accuracy) to trimodal fusion</li>
<li><strong>Dropout Regularization</strong>: 0.3 and 0.2 dropout rates to prevent overfitting in fusion layers</li>
</ul>

<h3>Video Processing Pipeline</h3>
<ul>
<li><strong>Frame Extraction</strong>: ~1 frame per second sampling with CVLib face detection</li>
<li><strong>Audio Extraction</strong>: MoviePy conversion to 16kHz WAV with Whisper transcription</li>
<li><strong>Feature Extraction</strong>: Parallel processing of vision (518×518), audio (768-dim), and text (512 tokens) features</li>
<li><strong>Multimodal Fusion</strong>: Cross-attention mechanism with concatenated feature fusion</li>
<li><strong>Emotion Classification</strong>: Softmax output across 5 emotion classes</li>
</ul>

<h3>Technical Stack</h3>
<ul>
<li><strong>Deep Learning</strong>: PyTorch 1.13.0+ with Transformers 4.21.0+</li>
<li><strong>Web Interface</strong>: Streamlit 1.24.0+ with custom CSS styling</li>
<li><strong>Computer Vision</strong>: OpenCV for face detection and image processing</li>
<li><strong>Audio Processing</strong>: Librosa 0.9.2+ and MoviePy 1.0.3+ for audio extraction</li>
<li><strong>Model Optimization</strong>: TensorFlow Lite compatibility with TIMM 0.6.7+</li>
</ul>

<h3>Emotion Categories Detected</h3>
<ul>
<li>Happy 😄</li>
<li>Surprise 😢</li>
<li>Angry 😠</li>
<li>Melancholy 😨</li>
<li>Neutral 😐</li>
</ul>

<h3>Performance Benchmarks</h3>
<ul>
<li><strong>Overall Accuracy</strong>: 61% on MELD dataset (competitive with SOTA DialogueRNN at 60.25%)</li>
<li><strong>Processing Speed</strong>: GPU (RTX 3050) processes videos in 10-20 seconds</li>
<li><strong>Memory Usage</strong>: ~4GB model loading, 6-7GB peak usage for large videos</li>
<li><strong>F1-Score</strong>: 0.59-0.63 macro average across emotion classes</li>
</ul>

<h3>User Interface and Deployment</h3>
<ul>
<li><strong>Web Application</strong>: Streamlit-based interface with glass morphism design</li>
<li><strong>Processing Modes</strong>: Lightweight, Balanced, and High Fidelity options</li>
<li><strong>Docker Deployment</strong>: One-command deployment with pre-built container</li>
<li><strong>Cloud Ready</strong>: Compatible with Streamlit Cloud, AWS, Azure, and Google Cloud Run</li>
<li><strong>Interactive Results</strong>: Real-time emotion visualization with confidence scores and probability distributions</li>
</ul>

<h3>Hardware Requirements</h3>
<ul>
<li><strong>Minimum</strong>: 8GB RAM, CPU-only processing (30-60 seconds per video)</li>
<li><strong>Recommended</strong>: 16GB RAM, NVIDIA GPU with 4GB+ VRAM</li>
<li><strong>Optimal</strong>: 32GB RAM, Modern GPU like RTX 3060+ (5-10 seconds per video)</li>
</ul>

<p>This project demonstrates the advancement of multimodal AI on edge devices, combining computer vision, natural language processing, and audio analysis into a comprehensive emotion recognition system with state-of-the-art performance.</p>"
                 data-github="https://github.com/Rishab27279/MoodyAI">
                
                <div class="project-image">
                    <img src="https://visagetechnologies.com/app/uploads/2022/07/Face-Analysis-emotion-estimation.webp" alt="AI Mood Detector">
                </div>
                
                <div class="project-content">
                    <div class="project-subtitle">Multimodal Sentiment AI</div>
                    <h3><i class="fa fa-smile"></i>Moody.AI</h3>
                    <p>Moody.AI is a cutting-edge multimodal emotion recognition system analyzing video content using computer vision, audio processing, and natural language processing technologies. Combining DINOv2, fine-tuned Wav2Vec2, DistilBERT, and Whisper models through innovative trimodal fusion architecture, it achieves 61% accuracy on challenging MELD dataset with Docker deployment..</p>
                </div>
            </div>
            <!-- OsteoDiagnosis.AI -->
            <div class="project-box scroll-animation"
                 data-popup-trigger="project-popup-1"
                 data-heading="OsteoDiagnosis.AI: Revolutionizing Bone Health Through Intelligent Mobile Diagnostics"
                 data-subheading="🦴  Next-Gen Bone Diagnostics: AI-Powered, Mobile-First, Research-Driven, Powered by Deep Learning.📱"
                 data-body="<style>
                            h3 {
  margin-bottom: 20px;
  margin-top: 20px;
  color: var(--ai-blue);
}
</style>
                            <p><strong>OsteoDiagnosis.AI</strong> is an innovative Android application that leverages advanced signal processing and deep learning techniques for automated bone health assessment. The app utilizes a novel, lightweight neural network architecture combining Signal Processing (Fourier) and Deep Learning to classify bone density conditions into three categories: Osteoporosis, Osteopenia, and Normal bone density. This research-driven project represents a significant advancement in mobile healthcare AI, combining our cutting-edge computer vision with clinical diagnostic applications.</p><p>----------------------------------------------------------------------------------------------------------------</p> <p>APK: https://github.com/Rishab27279/OS_Detection_Binary_And_3_Class_DWT/releases/download/v1.0/app-debug.apk</p><p>----------------------------------------------------------------------------------------------------------------</p> <h3>Research Context</h3> <p>This project was developed under academic supervision as part of ongoing research in medical AI diagnostics in BITS Pilani Hyderabad. While the complete technical methodology and results are currently confidential as the Research Paper is in it's last stages, the application demonstrates the successful integration of signal processing techniques with Modern deep learning architectures for bone health assessment.</p> <h3>Application Features</h3> <ul> <li>Three-class bone density classification (Osteoporosis, Osteopenia, Normal)</li> <li>Real-time inference capabilities on mobile devices</li> <li>Offline processing with no internet connectivity required</li> <li>Intuitive user interface designed for clinical workflow integration</li> <li>Optimized model architecture for mobile deployment efficiency (Model Size < 100Mb)</li> <li>High accuracy performance validated through rigorous testing protocols and Benchmarks (Accuracy ~ 90% ; Kappa Score ~ 0.82)</li> </ul> <h3>Technical Innovation</h3> <p>The core innovation lies in the fusion of traditional signal processing methodologies with state-of-the-art deep learning approaches:</p> <ul> <li><strong>Novel Architecture</strong>: Custom-designed neural network optimized for bone density classification tasks</li> <li><strong>Signal Processing Integration</strong>: Powerful Signal Processing techniques enhance feature extraction and model performance</li> <li><strong>Mobile Optimization (< 100 Mb)</strong>: Lightweight model design ensures efficient on-device inference</li> <li><strong>High Accuracy (~90%)</strong>: Achieved superior classification performance through innovative architectural design</li> </ul> <h3>Research Methodology</h3> <p>Due to the ongoing nature of this research and pending publication, specific technical details regarding the model architecture, training protocols, and validation datasets remain confidential. The methodology represents a novel contribution to the field of medical AI, particularly in bone health diagnostics.</p> <h3>Technical Implementation</h3> <ul> <li><strong>Android Studio</strong>: Native Android application development environment</li> <li><strong>Deep Learning Framework</strong>: Advanced neural network implementation for mobile deployment[4]</li> <li><strong>Signal Processing Libraries</strong>: Integrated preprocessing pipeline for enhanced feature extraction</li> <li><strong>Mobile Optimization</strong>: Efficient model compression and inference optimization techniques</li> </ul> <h3>Classification Categories</h3> <ul> <li><strong>Normal Bone Density</strong>: Healthy bone structure classification</li> <li><strong>Osteopenia</strong>: Mild bone density reduction detection</li> <li><strong>Osteoporosis</strong>: Severe bone density loss identification</li> </ul> <h3>Clinical Significance</h3> <ul> <li><strong>Early Detection</strong>: Enables timely identification of bone health deterioration</li> <li><strong>Accessibility</strong>: Mobile-first approach increases diagnostic accessibility</li> <li><strong>Efficiency</strong>: Rapid classification reduces diagnostic time and healthcare costs</li> <li><strong>Accuracy</strong>: High-performance model ensures reliable clinical decision support</li> </ul> <p>This project demonstrates the successful application of advanced AI techniques to critical healthcare challenges, showcasing the potential for mobile-deployed deep learning solutions in clinical diagnostics. The combination of academic rigor with practical implementation highlights the intersection of research innovation and real-world healthcare applications.</p>"
                 data-github="https://github.com/Rishab27279/OS_Detection_Binary_And_3_Class_DWT/releases/download/v1.0/app-debug.apk">
                
                <div class="project-image">
                    <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRvRe4ZssMsEzbdy5csZLpSjvljyaw4DsHc8q5MgaMUHUg0JfSOk5zl1Usc4sqeefrjO90&usqp=CAU" alt="OsteoDiagnosis AI Mobile App">
                </div>
                
                <div class="project-content">
                    <div class="project-subtitle">Novel Bone Health Diagnostics App</div>
                    <h3><i class="fa fa-bone"></i>OsteoDiagnosis.AI</h3>
                    <p>OsteoDiagnosis.AI (product of our Ongoing Research) is an innovative novel deep learning architecture integrating Advanced Signal Processing techniques for three-class Osteoporosis classification achieving 90% accuracy, successfully deployed as comprehensive Android application for accessible bone health diagnostics.</p>
                </div>
            </div>

            <!-- Expression.AI -->
            <div class="project-box scroll-animation"
                 data-popup-trigger="project-popup-2"
                 data-heading="Expression.AI — Decode Emotions in Real-Time"
                 data-subheading="🎭 Feel the Mood. Frame by Frame. Powered by Deep Learning. 🎭"
                 data-body="<style>
                            h3 {
  margin-bottom: 20px;
  margin-top: 20px;
  color: var(--ai-blue);
}
</style>
                            <p><strong>Expression.AI</strong> is a real-time facial expression recognition Android application that uses deep learning to detect and classify human emotions. Powered by a custom made TensorFlow Lite model named <strong>ResInceptionCNN</strong>, the app is designed for fast, on-device inference and intuitive user experience. It combines the power of computer vision with emotion AI for mobile devices.</p> <p>App APK -> https://github.com/Riiishaab/Expression.AI/releases/download/v1.0/ExpressionAI.apk</p> <h3>Application Features</h3> <ul> <li>Real-time facial emotion detection using the device's camera</li> <li>Supports classification from uploaded images via gallery</li> <li>Runs fully offline with no internet required for inference</li> <li>Emotion results displayed with matching emojis and labels</li> <li>Fast and efficient performance optimized for mobile usage</li> </ul> <h3>Model and Dataset</h3> <p>Expression.AI is built on a custom deep learning model trained for facial expression recognition:</p> <ul> <li><strong>Model Name</strong>: ResInceptionCNN — a hybrid architecture combining strengths of Residual and Inception networks</li> <li><strong>Dataset</strong>: Trained on the FER2013 dataset containing 48x48 grayscale facial images labeled with 7 emotion classes</li> <li><strong>Conversion</strong>: Exported to TensorFlow Lite format for mobile deployment</li> </ul> <h3>ResInceptionCNN Architecture</h3> <p>The core of the project is the ResInceptionCNN model:</p> <ul> <li><strong>Inception Blocks</strong>: Capture multi-scale spatial features for rich local and global context</li> <li><strong>Residual Connections</strong>: Enable deeper networks by addressing vanishing gradients and improving feature flow</li> <li><strong>Batch Normalization</strong>: Applied after each convolution to stabilize and speed up training</li> <li><strong>Dropout Layers</strong>: Reduce overfitting and improve generalization</li> <li><strong>Final Dense Layer</strong>: Outputs emotion class probabilities through a softmax activation</li> </ul> <h3>Technical Stack</h3> <ul> <li><strong>Android Studio</strong>: Used for developing the UI and app logic</li> <li><strong>TensorFlow Lite</strong>: Facilitates fast, on-device model inference</li> <li><strong>OpenCV</strong>: Handles face detection from camera and gallery inputs</li> <li><strong>CameraX</strong>: Provides real-time camera feed with face capture</li> </ul> <h3>Emotion Categories Detected</h3> <ul> <li>Angry 😠</li> <li>Disgust 😖</li> <li>Fear 😨</li> <li>Happy 😄</li> <li>Sad 😢</li> <li>Surprise 😲</li> <li>Neutral 😐</li> </ul> <h3>User Interface</h3> <ul> <li><strong>Live Camera View</strong>: Real-time emotion recognition with facial overlay</li> <li><strong>Image Upload</strong>: Choose from gallery for static image analysis</li> <li><strong>Emoji and Label Output</strong>: Emotion results shown with emojis and text</li> <li><strong>Minimal UI</strong>: Clean design with clear interaction flow and responsive layout</li> </ul> <p>This project demonstrates the potential of deep learning on edge devices, blending AI, mobile development, and human emotion understanding into a seamless Android experience.</p>"
                 data-github="https://github.com/Riiishaab/Expression.AI">
                
                <div class="project-image">
                    <img src="https://images.unsplash.com/photo-1512941937669-90a1b58e7e9c?ixlib=rb-4.0.3&auto=format&fit=crop&w=800&q=80" alt="Expression AI Mobile App">
                </div>
                
                <div class="project-content">
                    <div class="project-subtitle">Android APP POWERED BY AI</div>
                    <h3><i class="fa fa-smile"></i>Expression.AI</h3>
                    <p>Expression.AI is a real-time facial expression recognition Android app using TensorFlow Lite and OpenCV. It classifies emotions from camera or gallery images using a custom model trained on FER2013. Its fast, efficient and less than 500 Mb. You can download the app from the APK Link inside the Pop-Up. (*Note: Its Completely Virus free)</p>
                </div>
            </div>

            <!-- Additional projects would continue here with similar structure -->
            <!-- Hand Gesture Recognition -->
            <div class="project-box scroll-animation"
                 data-popup-trigger="project-popup-3"
                 data-heading="AI Hand Gesture Recognition System 🖐️"
                 data-subheading="🤟🏼 Real-time gesture recognition through FastViT Model ✌️"
                 data-body="<style>
                            h3 {
  margin-bottom: 20px;
  margin-top: 20px;
  color: var(--ai-blue);
}
</style>
                            <p>This project implements real-time hand gesture recognition using Apple's FastViT architecture, leveraging transfer learning on the HaGRID dataset to achieve 97.5% accuracy while maintaining efficiency for real-time applications.</p> <h3>🔑 Key Features</h3> <ul> <li><strong>Real-time gesture recognition</strong> through webcam integration</li> <li><strong>19 different hand gestures</strong> recognized with high accuracy</li> <li><strong>Efficient model architecture</strong> using FastViT for low-latency predictions</li> <li><strong>Simple deployment</strong> through Google Colab (no local setup required)</li> </ul> <h3>🧠 Model &amp; Architecture</h3> <p>The core intelligence is provided by a hybrid vision transformer:</p> <ul> <li><strong>Backbone</strong>: <code>fastvit_t8.apple_in1k</code> pretrained model</li> <li><strong>Training approach</strong>: Transfer learning with a frozen backbone</li> <li><strong>Input size</strong>: 256×256 px</li> <li><strong>Classes</strong>: 19 hand gestures</li> </ul> <p>FastViT was chosen for its efficiency advantages over ConvNeXT, offering high accuracy in resource-constrained environments.</p> <h3>📊 Dataset</h3> <p>Trained on the Hand Gesture Recognition Image Dataset (<strong>HaGRID</strong>) 150k subset:</p> <ul> <li><strong>19 gesture classes</strong> including 'thumbs up', 'peace sign', and 'stop'</li> <li>Used the manageable <strong>150k version</strong> for Colab training</li> <li>Properly split between <strong>training</strong> and <strong>validation</strong> sets</li> </ul> <h3>🚀 Usage</h3> <p><strong>Option 1: Google Colab</strong></p> <ul> <li>Open the training notebook</li> <li>Run all cells to train or load pretrained weights</li> <li>Follow webcam integration instructions</li> </ul> <p><strong>Option 2: Inference with Pretrained Model</strong></p> <ul> <li>Open the inference notebook</li> <li>Upload the pretrained model file (<code>sign_lang_model.pkl</code>)</li> <li>Run the webcam inference cell</li> </ul> <h3>📈 Results</h3> <ul> <li><strong>97.5% accuracy</strong> on the validation set</li> <li>Robust across different lighting conditions</li> <li><strong>Real-time inference</strong> (>30 FPS on modern hardware)</li> </ul> <h3>🔮 Future Work</h3> <ul> <li><strong>Expanded gesture vocabulary</strong>: Full sign language alphabet and phrases</li> <li><strong>Standalone deployment</strong>: Integration with video conferencing</li> <li><strong>Sequence modeling</strong>: Temporal information for dynamic gestures</li> <li><strong>Model optimization</strong>: Quantization and pruning for edge devices</li> </ul>"
                 data-github="https://github.com/Riiishaab/Sign_Language_Detector">
                
                <div class="project-image">
                    <img src="https://ai.google.dev/static/mediapipe/images/solutions/examples/hand_gesture.png" alt="Hand Gesture Recognition">
                </div>
                
                <div class="project-content">
                    <div class="project-subtitle">COMPUTER VISION with DEEP LEARNING</div>
                    <h3><i class="fa fa-hand-scissors"></i>Hand Gesture Identifier</h3>
                    <p>Developed an AI Hand Gesture Recognition System leveraging Apple's FastViT Vision Transformer, achieving 97.5% accuracy on the validation set. This project focuses on real-time gesture recognition for 19 distinct hand gestures, demonstrating efficient model performance for practical applications. The system utilizes transfer learning on the HaGRID dataset, making it suitable for deployment in resource-constrained environments.</p>
                </div>
            </div>


            <!-- AI Document Processing -->
            <div class="project-box scroll-animation"
                 data-popup-trigger="project-popup-4"
                 data-heading="AI Document Processing System 📄"
                 data-subheading="🚀 Intelligent Document Analysis and Summarization 🚀"
                 data-body="<style>
                            h3 {
  margin-bottom: 20px;
  margin-top: 20px;
  color: var(--ai-blue);
}
</style>
                            Developed an AI-driven document processing system combining DeepSeek R1-1.5B for structured data extraction and Llama-7B for summarization. Achieved 92% accuracy in entity extraction and 88% ROUGE-L summarization scores, enabling efficient processing of legal and technical documents."
                 data-github="https://github.com/Riiishaab/Projects/tree/main/AI%20Document%20Assistant">
                
                <div class="project-image">
                    <img src="https://writesonic.com/wp-content/uploads/AI-Agents-scaled.jpg" alt="AI Document Assistant">
                </div>
                
                <div class="project-content">
                    <div class="project-subtitle">NATURAL LANGUAGE PROCESSING Integrating RAG</div>
                    <h3><i class="fa fa-file-text"></i>AI Document Assistant</h3>
                    <p>Smart document processing system using DeepSeek and Llama models for intelligent analysis, extraction, and summarization of complex documents.</p>
                </div>
            </div>

            <!-- Smart AI Checkers -->
            <div class="project-box scroll-animation"
                 data-popup-trigger="project-popup-5"
                 data-heading="THE ULTIMATE CHECKERS AI"
                 data-subheading="🎮 Where Strategic Intelligence Conquers the Classic Game 🎮"
                 data-body="<style>
                            h3 {
  margin-bottom: 20px;
  margin-top: 20px;
  color: var(--ai-blue);
}
</style>
                            This code implements a comprehensive Checkers (Draughts) game featuring two AI players with contrasting strategies: a sophisticated Smart AI using the Minimax algorithm with alpha-beta pruning and a baseline Random AI that makes random legal moves.</p> <h3>Game Structure</h3> <ul> <li>The game utilizes an 8×8 checkerboard grid with traditional alternating dark and light squares</li> <li>Two AI players compete: Red (Smart AI) and Blue (Random AI)</li> <li>Turn-based gameplay with visual representation of each move</li> <li>King promotion mechanics when pieces reach the opponent's back row</li> <li>Standard checkers rules including mandatory captures and multiple jumps</li> </ul> <h3>AI Implementation</h3> <p>The Minimax algorithm implementation represents the core intelligence of the Smart AI:</p> <ul> <li><strong>Depth-Limited Search</strong>: The Smart AI (Red player) looks ahead 3 moves using the Minimax algorithm</li> <li><strong>Alpha-Beta Pruning</strong>: Optimizes the search by eliminating branches that won't affect the final decision</li> <li><strong>Recursive Evaluation</strong>: Each potential move is evaluated by simulating future game states</li> <li><strong>Heuristic Evaluation</strong>: Board positions are scored based on piece count, king status, and positioning</li> <li><strong>Maximizing Strategy</strong>: The AI selects moves that maximize its advantage while assuming optimal opponent play</li> </ul> <p>The Random AI provides a contrasting approach:</p> <ul> <li>Identifies all pieces with legal moves</li> <li>Randomly selects one of these pieces</li> <li>Randomly selects one of the legal moves for that piece</li> <li>Executes without strategic planning</li> </ul> <h3>Game Mechanics</h3> <ul> <li><strong>Move Validation</strong>: Comprehensive checking of legal moves including diagonal movement and jumps</li> <li><strong>Capture Logic</strong>: Implementation of mandatory jump rules and multiple capture sequences</li> <li><strong>Visual Representation</strong>: Board state displayed using HTML/CSS with color-coded pieces</li> <li><strong>Game Termination</strong>: Ends after 100 moves or when a player has no remaining pieces/moves</li> <li><strong>Winner Determination</strong>: Based on remaining piece count or inability of opponent to move</li> </ul> <h3>User Interface</h3> <ul> <li><strong>Interactive Display</strong>: Color-coded pieces with clear visual distinction between regular pieces and kings</li> <li><strong>Move Animation</strong>: Step-by-step visualization with appropriate delays for user comprehension</li> <li><strong>Notifications</strong>: Popup alerts for game start, end, and important events</li> <li><strong>Status Updates</strong>: Ongoing information about game progress, turn count, and current player</li> <li><strong>Board Rendering</strong>: Clear representation of the current game state after each move</li> </ul> <p>This implementation demonstrates advanced concepts in game AI development.</p>"
                 data-github="https://github.com/Riiishaab/Projects/blob/main/my_smart_ai_checkers_bot.py">
                
                <div class="project-image">
                    <img src="https://www.shubhom.com/img/checkersbackground.png" alt="AI Checkers Game">
                </div>
                
                <div class="project-content">
                    <div class="project-subtitle">AI Course Project</div>
                    <h3><i class="fa fa-robot"></i>Smart AI Checkers Bot</h3>
                    <p>A Checkers game featuring a Smart AI using Minimax with alpha-beta pruning against a Random AI, demonstrating strategic decision-making in game development.</p>
                </div>
            </div>

            <!-- Gender Detection -->
            <div class="project-box scroll-animation"
                 data-popup-trigger="project-popup-6"
                 data-heading="GenderNet Vision"
                 data-subheading="🔍 Advanced Facial Analysis System 🔍"
                 data-body="<style>
                            h3 {
  margin-bottom: 20px;
  margin-top: 20px;
  color: var(--ai-blue);
}
</style>
                            Implemented a gender detection system using hybrid CNN-InceptionV3 architecture, achieving 94.35% accuracy on the CelebA dataset. Features include dynamic augmentation, adaptive learning rate scheduling, and quantized TensorFlow Lite deployment."
                 data-github="https://github.com/Riiishaab/Projects/blob/main/Face%20Gender%20Detection%20using%20CNNs.ipynb">
                
                <div class="project-image">
                    <img src="https://media.gettyimages.com/photos/facial-recognition-technology-picture-id1139859279?k=6&m=1139859279&s=612x612&w=0&h=H-i0yAM3A49I__r44424-jACD667nxiKb7bZR52ByOA=" alt="Gender Detection System">
                </div>
                
                <div class="project-content">
                    <div class="project-subtitle">Course Project of Neural Network and Fuzzy Logic</div>
                    <h3><i class="fa fa-user"></i>Human Face Gender Detection</h3>
                    <p>Deep learning-based gender detection system using CNN & InceptionV3, achieving 94.35% accuracy on the CelebA dataset with advanced augmentation techniques.</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Skills Section -->
    <section id="skills" class="skills-section scroll-animation">
        <h2 class="section-title">Technical Arsenal</h2>
        <div class="skills-container">
            
            <!-- Programming Category -->
            <div class="skill-category scroll-animation">
                <h3>Programming Languages</h3>
                <div class="skills-grid">
                    <div class="skill-box">
                        <i class="fab fa-python" style="color: #3776ab;"></i>
                        <span class="skill-name">Python</span>
                    </div>
                    <div class="skill-box">
                        <i class="fas fa-code" style="color: #004482;"></i>
                        <span class="skill-name">C Programming</span>
                    </div>
                    <div class="skill-box">
                        <i class="fas fa-database" style="color: #f29111;"></i>
                        <span class="skill-name">SQL</span>
                    </div>
                    <div class="skill-box">
                        <i class="fab fa-git-alt" style="color: #f34f29;"></i>
                        <span class="skill-name">Git</span>
                    </div>
                    <div class="skill-box">
                        <i class="fas fa-microchip" style="color: #1a237e;"></i>
                        <span class="skill-name">Verilog</span>
                    </div>
                </div>
            </div>

            <!-- Frameworks & Libraries Category -->
            <div class="skill-category scroll-animation">
                <h3>AI/ML Frameworks</h3>
                <div class="skills-grid">
                    <div class="skill-box">
                        <i class="fas fa-fire" style="color: #EE4C2C;"></i>
                        <span class="skill-name">PyTorch</span>
                    </div>
                    <div class="skill-box">
                        <i class="fas fa-project-diagram" style="color: #FF6F00;"></i>
                        <span class="skill-name">TensorFlow</span>
                    </div>
                    <div class="skill-box">
                        <i class="fas fa-eye" style="color: #007BFF;"></i>
                        <span class="skill-name">Computer Vision</span>
                    </div>
                    <div class="skill-box">
                        <i class="fas fa-robot" style="color: #F7931E;"></i>
                        <span class="skill-name">Scikit-learn</span>
                    </div>
                    <div class="skill-box">
                        <i class="fas fa-feather" style="color: #D00000;"></i>
                        <span class="skill-name">Keras</span>
                    </div>
                    <div class="skill-box">
                        <i class="fas fa-cube" style="color: #013243;"></i>
                        <span class="skill-name">NumPy</span>
                    </div>
                    <div class="skill-box">
                        <i class="fas fa-table" style="color: #150458;"></i>
                        <span class="skill-name">Pandas</span>
                    </div>
                    <div class="skill-box">
                        <i class="fas fa-chart-line" style="color: #11557C;"></i>
                        <span class="skill-name">Matplotlib</span>
                    </div>
                    <div class="skill-box">
                        <i class="fab fa-docker" style="color: #2496ED;"></i>
                        <span class="skill-name">Docker</span>
                    </div>
                    <div class="skill-box">
                        <i class="fas fa-link" style="color: #1C3C3C;"></i>
                        <span class="skill-name">LangChain</span>
                    </div>
                    <div class="skill-box">
                        <i class="fas fa-bolt" style="color: #009688;"></i>
                        <span class="skill-name">FastAPI</span>
                    </div>
                    <div class="skill-box">
                        <i class="fab fa-android" style="color: #3DDC84;"></i>
                        <span class="skill-name">Android Studio</span>
                    </div>
                    <div class="skill-box">
                        <i class="fas fa-stream" style="color: #FF4B4B;"></i>
                        <span class="skill-name">Streamlit</span>
                    </div>
                </div>
            </div>

            <!-- Domain Skills Category -->
            <div class="skill-category scroll-animation">
                <h3>AI Specializations</h3>
                <div class="skills-grid">
                    <div class="skill-box">
                        <i class="fas fa-brain" style="color: #FFD700;"></i>
                        <span class="skill-name">Machine Learning</span>
                    </div>
                    <div class="skill-box">
                        <i class="fas fa-network-wired" style="color: #00FF00;"></i>
                        <span class="skill-name">Deep Learning</span>
                    </div>
                    <div class="skill-box">
                        <i class="fas fa-magic" style="color: #FF4500;"></i>
                        <span class="skill-name">Generative AI</span>
                    </div>
                    <div class="skill-box">
                        <i class="fas fa-comments" style="color: #1E90FF;"></i>
                        <span class="skill-name">LLM</span>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Education Section -->
    <section id="education" class="education-section scroll-animation">
        <h2 class="section-title">Education</h2>
        <div class="education-box scroll-animation">
            <h2 class="education-heading">Birla Institute of Technology and Science, Pilani</h2>
            <h3 class="degree-subheading">Bachelor of Engineering in Electronics and Communication Engineering</h3>
            <p class="location-text">Hyderabad, India • Currently Studying</p>
            
            <h4 class="coursework-heading" style="color: var(--netflix-red); margin: 2rem 0 1rem;">Relevant Coursework</h4>
            <ul class="coursework-list">
                <li class="coursework-item scroll-animation">Machine Learning for Electronics Engineer</li>
                <li class="coursework-item scroll-animation">Neural Network and Fuzzy Logic</li>
                <li class="coursework-item scroll-animation">Deep Learning</li>
                <li class="coursework-item scroll-animation">Digital Signal Processing</li>
                <li class="coursework-item scroll-animation">Signals and Systems</li>
                <li class="coursework-item scroll-animation">Probability and Statistics</li>
                <li class="coursework-item scroll-animation">Microprocessor and Interfacing</li>
                <li class="coursework-item scroll-animation">Linear Algebra</li>
            </ul>
        </div>
    </section>

  <!--  --------------------Blogs Section-------------------------------------------------------------------------->
<section id="blogs">
    <div>
      <h2 class="section-title">Blogs</h2>
        <div class="project-box scroll-animation" onclick="window.open('https://ai.gopubby.com/medmamba-explained-the-first-vision-mamba-for-generalized-medical-image-classification-is-3aee20a0751a', '_blank')" style="cursor: pointer;">
            <div class="project-image">
                <img src="https://miro.medium.com/v2/resize:fit:1100/format:webp/0*EdeQkbhru8Ub-SVf" alt="Blog Post Title">
            </div>
            
            <div class="project-content">
                <div class="project-subtitle">Deep Dive • Deep Learning • State-Space-Models • Transformers Replacement • Efficient Computation</div>
                <h3><i class="fa fa-edit"></i>MedMamba Explained : The first Vision Mamba for Generalized Medical Image Classification is finally here!!!  (Medium ~ AI Advances)</h3>
                <p>A blog on MedMamba, the first Vision Mamba architecture specifically designed for generalized medical image classification, addressing the computational limitations of traditional CNNs and Vision Transformers. Developed by Yubiao Yue and Zhenzhang Li, this groundbreaking work positions MedMamba as a superior replacement for ViTs by achieving linear computational complexity (O(N)) compared to ViTs' quadratic complexity (O(N²)), making it ideal for resource-constrained medical environments where real-time diagnosis is critical. The comprehensive technical guide explores how MedMamba leverages State Space Models (SSMs) and the innovative 2D-Selective-Scan mechanism to maintain global receptive fields while reducing FLOPs by up to 55% compared to equivalent transformer models. The blog demonstrates why practitioners should adopt MedMamba over traditional ViTs: it delivers competitive accuracy (93.7% average across medical datasets) with significantly lower memory requirements and faster inference times, making it practical for deployment in clinical settings where computational efficiency directly impacts patient care.</p>
            </div>
        </div>
    </div>
</section>

  
  
    <!-- ======================================== CHATBOT INTEGRATION ======================================== -->
    
    <!-- ChatBot Toggle Button -->
    <button class="chatbot-toggle" id="chatbotToggle" aria-label="Open AI Assistant">
        <i class="fas fa-robot"></i>
    </button>

    <!-- ChatBot Container -->
    <div class="chatbot-container" id="chatbotContainer">
        <!-- ChatBot Header -->
        <div class="chatbot-header">
            <h3 class="chatbot-title">Hi, I am Rishi-Bot 🤖</h3>
            <p class="chatbot-subtitle">I am Rishab's AI Avatar here to assist you in my Portfolio Journey!</p>
            <button class="chatbot-close" id="chatbotClose" aria-label="Close Chat">
                <i class="fas fa-times"></i>
            </button>
        </div>

        <!-- ChatBot Messages Area -->
        <div class="chatbot-messages" id="chatbotMessages">
            <!-- Messages will be dynamically inserted here -->
        </div>

        <!-- ChatBot Input Area -->
        <div class="chatbot-input-area">
            <div class="chatbot-input-container">
                <textarea 
                    class="chatbot-input" 
                    id="chatbotInput" 
                    placeholder="Ask about Rishab's projects, skills, experience..."
                    rows="1"></textarea>
                <button class="chatbot-send" id="chatbotSend" aria-label="Send Message">
                    <i class="fas fa-paper-plane"></i>
                </button>
            </div>
        </div>
    </div>

    <!-- Popup Containers (keeping all existing popups) -->
    <div id="experience-popup-1" class="popup">
        <div class="popup-content">
            <h2 class="popup-heading"></h2>
            <h3 class="popup-subheading"></h3>
            <div class="popup-body"></div>
            <div class="popup-buttons">
                <a href="" class="popup-github" target="_blank">
                    <i class="fab fa-github"></i> Code
                </a>
                <button class="popup-close">Close</button>
            </div>
        </div>
    </div>

    <div id="experience-popup-2" class="popup">
        <div class="popup-content">
            <h2 class="popup-heading"></h2>
            <h3 class="popup-subheading"></h3>
            <div class="popup-body"></div>
            <div class="popup-buttons">
                <a href="" class="popup-github" target="_blank">
                    <i class="fab fa-github"></i> Code
                </a>
                <button class="popup-close">Close</button>
            </div>
        </div>
    </div>

    <div id="experience-popup-3" class="popup">
        <div class="popup-content">
            <h2 class="popup-heading"></h2>
            <h3 class="popup-subheading"></h3>
            <div class="popup-body"></div>
            <div class="popup-buttons">
                <a href="" class="popup-github" target="_blank">
                    <i class="fab fa-github"></i> Code
                </a>
                <button class="popup-close">Close</button>
            </div>
        </div>
    </div>

    <div id="research-popup-1" class="popup">
        <div class="popup-content">
            <h2 class="popup-heading"></h2>
            <h3 class="popup-subheading"></h3>
            <div class="popup-body"></div>
            <div class="popup-buttons">
                <a href="" class="popup-github" target="_blank">
                    <i class="fab fa-github"></i> View on GitHub
                </a>
                <button class="popup-close">Close</button>
            </div>
        </div>
    </div>

    <div id="research-popup-2" class="popup">
        <div class="popup-content">
            <h2 class="popup-heading"></h2>
            <h3 class="popup-subheading"></h3>
            <div class="popup-body"></div>
            <div class="popup-buttons">
                <a href="" class="popup-github" target="_blank">
                    <i class="fab fa-github"></i> No Code!
                </a>
                <button class="popup-close">Close</button>
            </div>
        </div>
    </div>

    <!-- Project popups -->
    <div id="project-popup-1" class="popup">
        <div class="popup-content">
            <h2 class="popup-heading"></h2>
            <h3 class="popup-subheading"></h3>
            <div class="popup-body"></div>
            <div class="popup-buttons">
                <a href="" class="popup-github" target="_blank">
                    <i class="fab fa-github"></i> Get APK
                </a>
                <button class="popup-close">Close</button>
            </div>
        </div>
    </div>  
  
    <div id="project-popup-2" class="popup">
        <div class="popup-content">
            <h2 class="popup-heading"></h2>
            <h3 class="popup-subheading"></h3>
            <div class="popup-body"></div>
            <div class="popup-buttons">
                <a href="" class="popup-github" target="_blank">
                    <i class="fab fa-github"></i> View on GitHub
                </a>
                <button class="popup-close">Close</button>
            </div>
        </div>
    </div>

    <div id="project-popup-3" class="popup">
        <div class="popup-content">
            <h2 class="popup-heading"></h2>
            <h3 class="popup-subheading"></h3>
            <div class="popup-body"></div>
            <div class="popup-buttons">
                <a href="" class="popup-github" target="_blank">
                    <i class="fab fa-github"></i> View on GitHub
                </a>
                <button class="popup-close">Close</button>
            </div>
        </div>
    </div>

    <div id="project-popup-4" class="popup">
        <div class="popup-content">
            <h2 class="popup-heading"></h2>
            <h3 class="popup-subheading"></h3>
            <div class="popup-body"></div>
            <div class="popup-buttons">
                <a href="" class="popup-github" target="_blank">
                    <i class="fab fa-github"></i> Code
                </a>
                <button class="popup-close">Close</button>
            </div>
        </div>
    </div>
  
    <div id="project-popup-5" class="popup">
        <div class="popup-content">
            <h2 class="popup-heading"></h2>
            <h3 class="popup-subheading"></h3>
            <div class="popup-body"></div>
            <div class="popup-buttons">
                <a href="" class="popup-github" target="_blank">
                    <i class="fab fa-github"></i> Code
                </a>
                <button class="popup-close">Close</button>
            </div>
        </div>
    </div>
  
    <div id="project-popup-6" class="popup">
        <div class="popup-content">
            <h2 class="popup-heading"></h2>
            <h3 class="popup-subheading"></h3>
            <div class="popup-body"></div>
            <div class="popup-buttons">
                <a href="" class="popup-github" target="_blank">
                    <i class="fab fa-github"></i> Code
                </a>
                <button class="popup-close">Close</button>
            </div>
        </div>
    </div>
  
    <div id="project-popup-8" class="popup">
        <div class="popup-content">
            <h2 class="popup-heading"></h2>
            <h3 class="popup-subheading"></h3>
            <div class="popup-body"></div>
            <div class="popup-buttons">
                <a href="" class="popup-github" target="_blank">
                    <i class="fab fa-github"></i> Code
                </a>
                <button class="popup-close">Close</button>
            </div>
        </div>
    </div>
  
      <div id="project-popup-7" class="popup">
        <div class="popup-content">
            <h2 class="popup-heading"></h2>
            <h3 class="popup-subheading"></h3>
            <div class="popup-body"></div>
            <div class="popup-buttons">
                <a href="" class="popup-github" target="_blank">
                    <i class="fab fa-github"></i> Code
                </a>
                <button class="popup-close">Close</button>
            </div>
        </div>
    </div>  

    <footer class="site-footer">
        <p>🤟🏼© Made by Rishab with passion for AI⚡</p>
    </footer>

    <!-- Enhanced JavaScript -->
    <script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script><script src="script.js"></script>
</body>
</html>
